# S3 (Simple Storage Service) — Deep, Pro‑Level Notes (US‑East‑1, USD)

## Who this is for
- **Beginner**: learn what S3 is, how to store files safely, and how to control costs.
- **AWS Developer**: build secure uploads/downloads, integrate with apps, and automate lifecycle.
- **Solutions Architect**: design scalable, compliant, cost‑optimized storage systems.

## Learning outcomes
After reading this file, you should be able to:
- Choose the correct storage class with confidence.
- Build a secure, private bucket with proper encryption and access control.
- Explain trade‑offs between S3, EBS, EFS, FSx, and Glacier.
- Calculate storage + request + retrieval costs and optimize them.
- Answer tricky interview questions with real‑world reasoning.

## 1) Why S3 is “best” (and when it is not)
### S3 is best when you need
- **Massive scale + durability**: 11 9s durability with multi‑AZ replication built in.
- **Simple global access**: HTTP(S) access, SDKs, pre‑signed URLs.
- **Low ops overhead**: no servers, automatic scaling.
- **Flexible cost control**: multiple storage classes + lifecycle rules.

### S3 is not best when you need
- **Block storage** for OS or databases on EC2 → use **EBS**.
- **Shared POSIX file system** with low latency across many EC2s → use **EFS** or **FSx**.
- **Ultra‑low latency within a single AZ** for high‑IOPS workloads → use **FSx** or local NVMe.

### Compare S3 with similar services
- **S3 vs EBS**: S3 is object storage (HTTP APIs, eventual directory semantics), EBS is block storage (mounted disks, low latency). Use EBS for databases/OS, S3 for backups, data lakes, static assets.
- **S3 vs EFS**: EFS is shared NFS for Linux with file locking. S3 is object storage without file locks or POSIX semantics.
- **S3 vs FSx**: FSx is enterprise file storage (Windows SMB / Lustre / NetApp / OpenZFS). Use FSx when you need file system semantics, not object.
- **S3 vs Glacier**: Glacier classes are **part of S3** (archival storage classes). You still use S3 APIs, but retrieval can take minutes to hours.

### Mental model (beginner‑friendly)
Think of S3 as a **global‑scale hard drive** you access over the internet, but:
- You **don’t mount it** like a disk.
- Files are stored as **objects** (key + data + metadata).
- It is **highly durable** and **infinitely scalable** by design.

### How S3 scales “infinitely” (easy explanation)
S3 does not run on a single server. It uses a **massively distributed** storage system:
- Data is **sharded** across many storage nodes.
- New capacity is **added behind the scenes** without you doing anything.
- You don’t pre‑provision storage; S3 automatically accepts more objects as you upload.

### Internal architecture (conceptual, not proprietary)
AWS does not publish full internals, but the general design is:
- **Metadata layer** tracks buckets, object keys, versions, and permissions.
- **Data layer** stores object data in multiple AZs for durability.
- **Request routing layer** directs PUT/GET to the right storage nodes.

**Why this matters**  
- High durability because multiple copies exist across AZs.  
- High scalability because metadata and data are distributed.  
- No capacity planning is needed.

## 2) Core concepts you must master
- **Bucket**: region‑scoped container.
- **Object**: key + data + metadata.
- **Prefix**: naming convention only (not folders).
- **Consistency**: read‑after‑write for new and overwritten objects.
- **Versioning**: protects against deletes/overwrites.
 - **Object metadata**: system‑defined (e.g., content‑type) and user‑defined (custom headers).
 - **ETag**: usually a checksum for single‑part uploads; used for integrity checks.
 - **Bucket policy vs ACL**: policy is preferred; ACLs are legacy.
 - **Access Points**: create per‑app access boundaries without new buckets.
 - **S3 Event Notifications**: trigger Lambda/SNS/SQS on object events.
 - **Storage class analysis**: helps decide lifecycle moves based on access patterns.

### Simple key structure example
```
s3://my-bucket/
  images/2026/01/profile-123.png
  logs/2026/01/30/app.log
```
**Tip**: Use predictable prefixes to make lifecycle and analytics easier.

## 3) Storage classes: what, why, when
### How to choose (decision logic)
1) **Need instant access (ms)?**
   - **Frequent** → S3 Standard.
   - **Infrequent** → Standard‑IA (multi‑AZ) or One Zone‑IA (single‑AZ, cheaper).
   - **Rare but instant** → Glacier Instant Retrieval.
2) **Can wait minutes‑hours?** → Glacier Flexible Retrieval.
3) **Can wait hours‑days?** → Glacier Deep Archive.

### Minimum size & duration rules (exam‑critical)
- Standard‑IA & One Zone‑IA: **min 128 KB**, **min 30 days**.
- Glacier Instant Retrieval: **min 128 KB**, **min 90 days**.
- Glacier Flexible Retrieval: **min 90 days**.
- Glacier Deep Archive: **min 180 days**.
- Glacier Flexible & Deep Archive charge **40 KB metadata per object**.

### Beginner summary table
| Class | Access speed | Typical use | Key caution |
|---|---|---|---|
| Standard | ms | Hot data | Higher cost |
| Standard‑IA | ms | Infrequent access | 30‑day minimum |
| One Zone‑IA | ms | Infrequent, OK with 1‑AZ | Single‑AZ risk |
| Glacier IR | ms | Rare access but instant | 90‑day minimum |
| Glacier Flexible | minutes‑hours | Archive | Restore time |
| Glacier Deep Archive | hours‑days | Long‑term archive | Long restore time |

## 4) Lifecycle policies (deep)
### Why lifecycle matters
- Avoid paying “hot” prices for cold data.
- Avoid early deletion penalties in IA/Glacier.

### Example lifecycle (safe default)
- Days 0‑30: Standard
- Days 31‑365: Standard‑IA
- Day 366+: Glacier Deep Archive

### Edge cases to watch
- **Small objects (<128 KB)**: charged as 128 KB in IA/Glacier IR.
- **Frequent updates**: IA classes charge early‑deletion penalties.
- **Transition costs**: lifecycle transitions are billed per request.

## 5) Security deep dive (FAANG‑level)
### The access layers
1) **IAM policy** (principal) — who can call S3 APIs.
2) **Bucket policy** (resource) — what can access the bucket.
3) **Block Public Access** — global guardrail; keep enabled.
4) **KMS key policy** — if SSE‑KMS used, key policy controls access.

### Access flow diagram (ASCII)
```
User/App Role
   │  (IAM Policy allows)
   ▼
S3 Bucket Policy allows?
   │
   ▼
Block Public Access? (must not block)
   │
   ▼
KMS Key Policy allows? (if SSE‑KMS)
   │
   ▼
Access granted
```

### Encryption choices
- **SSE‑S3**: simplest, AWS‑managed keys.
- **SSE‑KMS**: audit trails + fine‑grained key control.
- **Client‑side**: you control keys, highest responsibility.

### Pro tips
- Use **OAC** for CloudFront‑to‑S3 private access.
- Use **Access Analyzer** to detect unintended public access.
- Enable **S3 Object Lock** + versioning for compliance.

### Common beginner mistakes
- Leaving Block Public Access off.
- Using ACLs instead of bucket policies.
- Forgetting to allow KMS permissions when using SSE‑KMS.

## 6) Performance & scaling
- S3 scales automatically; use **multipart upload** for large objects.
- Avoid tiny objects in archival classes (metadata overhead + minimum size).
- Use **S3 Transfer Acceleration** for global uploads (extra cost).

### Recommended upload strategy
- < 100 MB: single PUT is fine.
- 100 MB–5 GB: multipart upload recommended.
- > 5 GB: multipart upload required.

## 6.1) How S3 can serve a website directly
### What “static website hosting” means
S3 can serve **static content** (HTML/CSS/JS/images) directly using a bucket as a website origin.

### How it works (simple flow)
1) You upload static files to S3.
2) You enable **Static Website Hosting** on the bucket.
3) S3 generates a **website endpoint** that serves your index.html and error pages.

### Key limitations (important)
- S3 website endpoints are **HTTP only** (no HTTPS).  
- For HTTPS, use **CloudFront** in front of S3.  
- No server‑side code runs in S3; it is static only.

### Best practice
- Use **S3 + CloudFront + OAC** for secure, fast, HTTPS websites.  

## 7) Pricing model (what you pay for)
1) **Storage** (GB‑month).
2) **Requests** (PUT/GET/LIST, per 1,000).
3) **Retrieval** (GB read from IA/Glacier classes).
4) **Data transfer out** (internet egress; first 100 GB/month free across AWS services).
5) **Other**: lifecycle transitions, replication, analytics, inventory, etc.

### Authoritative notes from AWS
- S3 pricing page outlines charges, minimums, and data transfer rules.  
- Data transfer OUT to the internet is free for the first 100 GB/month (aggregate) in most regions.

## 8) US‑East‑1 pricing quick reference (examples)
> **Important**: The rates below are common US‑East‑1 figures published in AWS materials. **Always verify** in the AWS S3 pricing page or the Price List API before exams/interviews or production work.

### Storage cost per month (approx)
(Assume 1 GB = 1024 MB)

| Class | $/GB‑month | 1 MB | 100 MB | 1 GB |
|---|---:|---:|---:|---:|
| Standard | 0.023 | 0.000022 | 0.00225 | 0.023 |
| Standard‑IA | 0.0125 | 0.000012 | 0.00122 | 0.0125 |
| One Zone‑IA | 0.0100 | 0.0000098 | 0.00098 | 0.0100 |
| Glacier Instant Retrieval | 0.0040 | 0.0000039 | 0.00039 | 0.0040 |
| Glacier Flexible Retrieval | 0.0036 | 0.0000035 | 0.00035 | 0.0036 |
| Glacier Deep Archive | 0.00099 | 0.00000097 | 0.000096 | 0.00099 |

### Intelligent‑Tiering notes
- Storage charged per tier (Frequent/IA/Archive). 
- Monitoring fee per 1,000 objects/month.

### Beginner pricing intuition
- **Storage** is the main cost for large datasets.
- **Requests** matter when you have billions of small objects.
- **Retrieval** matters for IA and Glacier classes.

## 8.1) Advanced S3 features (certification‑critical)

### S3 Batch Operations
**What**: Execute large‑scale operations (copy, tag, restore, invoke Lambda) on billions of objects.

**When to use**:
- Encrypt existing unencrypted objects
- Copy objects across buckets/accounts
- Restore Glacier objects in bulk
- Tag objects for lifecycle or billing

**How it works**:
1. Create **S3 Inventory** report (CSV/Parquet)
2. Create Batch Operations job with manifest
3. S3 executes operations with retry and status tracking
4. Review completion report

**Pricing**: Per‑object operation cost + S3 request costs

> [!TIP]
> Use S3 Batch Operations for bulk remediation tasks instead of writing custom scripts. It scales better and provides built‑in retry logic.

### S3 Inventory
**What**: Automated daily/weekly reports of all objects in a bucket.

**Output formats**: CSV, ORC, Parquet

**Use cases**:
- Audit encryption status
- Track object age for lifecycle planning
- Generate manifests for Batch Operations
- Cost analysis and billing reports

**Example inventory configuration** (CLI):
```bash
aws s3api put-bucket-inventory-configuration \
  --bucket my-bucket \
  --id daily-inventory \
  --inventory-configuration '{
    "Destination": {
      "S3BucketDestination": {
        "Bucket": "arn:aws:s3:::inventory-bucket",
        "Format": "Parquet"
      }
    },
    "IsEnabled": true,
    "Schedule": {"Frequency": "Daily"},
    "IncludedObjectVersions": "Current",
    "OptionalFields": ["Size","StorageClass","ETag","EncryptionStatus"]
  }'
```

### S3 Storage Class Analysis
**What**: Tracks access patterns to recommend lifecycle transitions.

**How it works**:
- Monitors GET/HEAD requests for objects
- Generates recommendations after 30+ days
- Exports data to S3 for further analysis

**Best practice**: Run analysis for 30‑90 days before creating lifecycle rules.

### S3 Select and Glacier Select
**What**: Query CSV/JSON/Parquet data using SQL **without downloading** the entire object.

**Performance gain**: Up to **80% cost reduction** and **400% faster** than downloading.

**Example** (query CSV in S3):
```python
import boto3

s3 = boto3.client('s3')
resp = s3.select_object_content(
    Bucket='my-bucket',
    Key='data.csv',
    ExpressionType='SQL',
    Expression="SELECT * FROM s3object s WHERE s.age > 30",
    InputSerialization={'CSV': {"FileHeaderInfo": "Use"}},
    OutputSerialization={'JSON': {}}
)
```

**Exam tip**: S3 Select reduces data transfer costs; use it for filtering large datasets.

### S3 Access Points
**What**: Named endpoints that simplify access to shared buckets with per‑application policies.

**Why use Access Points**:
- Separate access policies per application
- Simpler policy management than massive bucket policies
- Supports VPC‑only access

**Architecture pattern**:
```
Bucket: company-data
  ├─ Access Point: finance-ap (policy: s3://bucket/finance/*)
  ├─ Access Point: hr-ap (policy: s3://bucket/hr/*)
  └─ Access Point: analytics-ap (VPC-only, read-only)
```

**Example** (create access point):
```bash
aws s3control create-access-point \
  --account-id 123456789012 \
  --name finance-access-point \
  --bucket company-data \
  --vpc-configuration VpcId=vpc-123456
```

**Access via Access Point**:
```
arn:aws:s3:us-east-1:123456789012:accesspoint/finance-access-point/object/report.pdf
```

> [!IMPORTANT]
> Access Points do NOT replace bucket policies; they layer additional policies on top. The bucket policy must still allow the Access Point.

### S3 Object Lambda
**What**: Transform objects on‑the‑fly during GET requests using Lambda.

**Use cases**:
- Redact PII before returning data
- Resize images dynamically
- Convert formats (XML to JSON)
- Enrich data with metadata

**How it works**:
1. Client requests object via **S3 Object Lambda Access Point**
2. S3 invokes Lambda with object data
3. Lambda transforms and returns modified data
4. Client receives transformed object

**Pricing**: Lambda invocation + S3 requests + data transfer

> [!WARNING]
> S3 Object Lambda adds latency (Lambda cold start + processing time). Use for transformations that can't be done at write time.

## 9) Request & retrieval costs (how to think)
### Requests
- PUT/LIST/POST and GET are billed per 1,000 requests.
- IA and Glacier classes have higher request costs than Standard.

### Retrieval
- **Standard‑IA & One Zone‑IA**: retrieval per GB read.
- **Glacier classes**: retrieval per GB + restore tier cost/time.

## 9.1) Monitoring, observability, and troubleshooting (production‑critical)

### CloudWatch metrics for S3
**Key S3 metrics** (automatic, 1-day delay):
- **BucketSizeBytes**: Total storage per class
- **NumberOfObjects**: Object count
- **AllRequests**, **GetRequests**, **PutRequests**: Request counts
- **4xxErrors**, **5xxErrors**: Error rates
- **FirstByteLatency**, **TotalRequestTime**: Performance

**Request metrics** (near real-time, opt-in, extra cost):
- Enable per bucket or prefix
- 1-minute CloudWatch visibility
- Filterable by prefix or tag

**Example** (enable request metrics via CLI):
```bash
aws s3api put-bucket-metrics-configuration \
  --bucket my-bucket \
  --id EntireBucket \
  --metrics-configuration '{"Id":"EntireBucket"}'
```

### CloudTrail for S3 (audit and security)
**What it logs**:
- **Management events** (bucket creation, policy changes) — free
- **Data events** (object PUT/GET/DELETE) — paid

**Best practices**:
- Enable data events for sensitive buckets
- Send logs to centralized audit account
- Use **CloudTrail Insights** for anomaly detection

**Example suspicious patterns to monitor**:
- Unexpected `DeleteObject` spikes
- `GetObject` from unknown IPs
- Bucket policy changes from non-admin users

**Query with Athena**:
```sql
SELECT eventname, sourceipaddress, COUNT(*) as count
FROM cloudtrail_logs
WHERE eventname LIKE '%Object%'
  AND errorcode IS NOT NULL
GROUP BY eventname, sourceipaddress
ORDER BY count DESC
```

### S3 Server Access Logs
**What**: Detailed request logs delivered to an S3 bucket.

**Log fields**: requester, bucket, key, operation, HTTP status, error code, turn-around time.

**Use cases**:
- Debug access denied errors
- Analyze traffic patterns
- Security forensics
- Billing analysis

**Enable logging**:
```bash
aws s3api put-bucket-logging \
  --bucket my-bucket \
  --bucket-logging-status '{
    "LoggingEnabled": {
      "TargetBucket": "my-logs-bucket",
      "TargetPrefix": "s3-access-logs/"
    }
  }'
```

> [!CAUTION]
> Never set the logging target to the same bucket being logged. This creates an infinite loop of log generation.

### Common troubleshooting scenarios

#### Scenario 1: 403 Access Denied on GetObject
**Checklist**:
1. IAM policy allows `s3:GetObject` on the object ARN?
2. Bucket policy does NOT deny the principal?
3. Block Public Access is NOT blocking (if public access intended)?
4. If SSE-KMS: KMS key policy allows `kms:Decrypt`?
5. Object ACL allows access (if ACLs enabled)?
6. VPC endpoint policy allows the action (if accessed via VPC endpoint)?

**Pro tip**: Use **IAM Policy Simulator** to test combined policies.

#### Scenario 2: Slow S3 uploads/downloads
**Diagnosis steps**:
- Check **network bandwidth** between client and S3
- Use **S3 Transfer Acceleration** if uploading from distant regions
- Use **multipart upload** for files >100 MB
- Check **CloudWatch FirstByteLatency** metric

**Fix**:
- Enable Transfer Acceleration
- Use parallel multipart uploads (AWS SDK default)
- Use CloudFront for frequent downloads

#### Scenario 3: Unexpected high costs
**Investigation**:
1. Check S3 CloudWatch **BucketSizeBytes** by storage class
2. Review **request metrics** for PUT/GET/LIST spikes
3. Check **data transfer out** (inter-region, internet egress)
4. Use **S3 Storage Lens** for cost insights
5. Check for accidental **small object proliferation** in IA/Glacier

**Common causes**:
- Many small objects in IA classes (128 KB minimum billing)
- Frequent lifecycle transitions
- High request rate from poorly optimized apps
- Unnecessary cross-region replication

### S3 Storage Lens (advanced analytics)
**What**: Organization-wide S3 visibility with metrics, trends, and recommendations.

**Features**:
- Cross-account aggregation
- 30+ metrics (storage, activity, cost efficiency)
- Free tier: 14 days retention, daily metrics
- Advanced tier: 15 months retention, prefix-level metrics, CloudWatch publishing

**Key metrics**:
- **Retrieval rate**: % of storage retrieved per month
- **Cost optimization**: % of incomplete multipart uploads
- **Security**: % of buckets with encryption, versioning, public access

**Example dashboard use cases**:
- Identify buckets without lifecycle rules
- Find buckets with unencrypted objects
- Track storage growth trends
- Compare request costs across teams

## 10) Hands‑on lab (pro‑level)
### Goal: Secure, optimized S3 bucket
1) Create bucket in `us‑east‑1`.
2) Enable **Block Public Access** + **Versioning**.
3) Set default **SSE‑KMS** encryption.
4) Create lifecycle: Standard → Standard‑IA at 30 days → Deep Archive at 365 days.
5) Create CRR to another region for DR.
6) Test access via IAM role (no static keys).

### Validate your setup (beginner checklist)
- Upload an object and confirm **versioning** creates versions.
- Confirm **default encryption** by checking object properties.
- Try a **public GET** and confirm it is blocked.
- Move the object forward in lifecycle (use a short test rule).

## 9.2) Disaster recovery and backup strategies (architect‑level)

### RTO and RPO with S3
**Recovery Time Objective (RTO)**: How fast can you restore access?
**Recovery Point Objective (RPO)**: How much data can you afford to lose?

| Strategy | RPO | RTO | Cost | Complexity |
|---|---|---|---|---|
| **Versioning only** | 0 (continuous) | Minutes | Low | Low |
| **Same-region replication (SRR)** | Seconds | Minutes | Medium | Low |
| **Cross-region replication (CRR)** | Seconds | Minutes | Medium-High | Medium |
| **CRR + separate account** | Seconds | Minutes-Hours | High | High |
| **Backup to Glacier** | Hours-Days | Hours-Days | Low | Low |

### Replication deep dive

#### Cross-Region Replication (CRR)
**Use cases**:
- Disaster recovery in different geographic region
- Comply with data residency requirements
- Reduce latency for global users

**Requirements**:
- Versioning enabled on source and destination
- Appropriate IAM role with trust relationship
- If using SSE-KMS: grant replication role access to both keys

**Replication Time Control (RTC)**:
- SLA: 99.99% of objects replicated within 15 minutes
- Additional cost: $0.015/GB
- Use for compliance/DR with strict time requirements

**What gets replicated**:
- ✅ New objects after replication is enabled
- ✅ Metadata, ACLs, tags
- ✅ Object Lock retention info
- ❌ Objects that existed before enabling replication (use Batch Operations)
- ❌ Glacier-archived objects (must restore first)
- ❌ Objects encrypted with SSE-C (use SSE-KMS or SSE-S3)

**Two-way (bidirectional) replication**:
```bash
# Enable both directions for active-active scenarios
Bucket A (us-east-1) ⇄ Bucket B (eu-west-1)
```

> [!WARNING]
> Bidirectional replication can amplify costs. Monitor replication metrics carefully.

#### Same-Region Replication (SRR)
**Use cases**:
- Log aggregation from multiple accounts
- Separate production and test data
- Compliance: maintain separate copies with different encryption

**Example architecture**:
```
Production account bucket (SSE-KMS) 
   → Compliance account bucket (SSE-KMS with different key)
```

### Multi-region architecture patterns

#### Pattern 1: Active-Passive DR
```merm aid
graph LR
    A[Primary: us-east-1] -->|CRR| B[DR: us-west-2]
    A -->|Route 53 Primary| C[Users]
    B -.->|Route 53 Failover| C
```

**Implementation**:
- CRR from primary to DR bucket
- Route 53 health check on primary region
- Automatic failover on health check failure
- Applications read from region-specific endpoints

#### Pattern 2: Active-Active Multi-Region
```
Users (Global)
   ↓
Route 53 (Latency-based routing)
   ↓
┌─────────────────┬─────────────────┐
│ us-east-1       │ S3 bucket A     │
│ S3 bucket A     │ S3 bucket B     │
└─────────────────┴─────────────────┘
         ↕ Bidirectional CRR ↕
```

**Use cases**: Global SaaS with low-latency reads

## 9.3) Integration patterns with AWS services (real-world architectures)

### S3 + Lambda (Event-Driven Processing)
**Trigger types**:
- `s3:ObjectCreated:*` — new uploads
- `s3:ObjectRemoved:*` — deletions
- `s3:ObjectRestore:*` — Glacier restore completion

**Common patterns**:
1. **Image thumbnail generation**
   ```
   S3 (upload) → Lambda → Resize → S3 (thumbnails bucket)
   ```

2. **Data validation and enrichment**
   ```
   S3 (raw CSV) → Lambda → Validate → DynamoDB + SNS
   ```

3. **Virus scanning**
   ```
   S3 (upload) → Lambda → ClamAV → Tag object or quarantine
   ```

**Best practices**:
- Use **S3 Event Notifications** to SQS/SNS for fan-out (avoid concurrent Lambda invocations on same object)
- Set **reserved concurrency** on Lambda to control cost
- Use **S3 batch operations** for backfilling existing objects

> [!TIP]
> For high-volume uploads, use **EventBridge** instead of direct S3 notifications for better filtering and routing to multiple targets.

### S3 + Athena + Glue (Data Lake)
**Architecture**:
```
Data Sources → S3 (Parquet/ORC with partitions)
                 ↓
             AWS Glue Crawler (schema discovery)
                 ↓
             Glue Data Catalog
                 ↓
             Amazon Athena (SQL queries)
```

**Optimization tips**:
- Partition by date: `s3://bucket/year=2026/month=01/day=30/`
- Use columnar formats (Parquet, ORC) for 10x cost savings
- Compress data (gzip, Snappy) to reduce scan costs
- Use **Athena CTAS** (Create Table As Select) for query result reuse

**Cost example** (1 TB query):
- JSON uncompressed: $5 per TB scanned
- Parquet compressed: ~$0.50 (10x data reduction)

### S3 + CloudFront (Global Content Delivery)
**Origin Access Control (OAC)** - modern best practice:
```bash
# 1. Create OAC
aws cloudfront create-origin-access-control \
  --origin-access-control-config '{
    "Name": "S3-OAC",
    "SigningBehavior": "always",
    "SigningProtocol": "sigv4",
    "OriginAccessControlOriginType": "s3"
  }'

# 2. Update bucket policy to allow only OAC
{
  "Version": "2012-10-17",
  "Statement": [{
    "Sid": "AllowCloudFrontOAC",
    "Effect": "Allow",
    "Principal": {"Service": "cloudfront.amazonaws.com"},
    "Action": "s3:GetObject",
    "Resource": "arn:aws:s3:::my-bucket/*",
    "Condition": {
      "StringEquals": {
        "AWS:SourceArn": "arn:aws:cloudfront::123456789012:distribution/E1234"
      }
    }
  }]
}
```

**Why OAC over OAI**:
- Supports SSE-KMS encryption
- Supports all S3 regions
- Better security (IAM-based, not long-lived credentials)

### S3 + Step Functions (Orchestration)
**Example workflow**: Video processing pipeline
```json
{
  "StartAt": "CheckVideoFormat",
  "States": {
    "CheckVideoFormat": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:FUNCTION",
      "Next": "IsValidFormat"
    },
    "IsValidFormat": {
      "Type": "Choice",
      "Choices": [{
        "Variable": "$.valid",
        "BooleanEquals": true,
        "Next": "TranscodeVideo"
      }],
      "Default": "NotifyError"
    },
    "TranscodeVideo": {
      "Type": "Task",
      "Resource": "arn:aws:states:::mediaconvert:createJob.sync",
      "Next": "GenerateThumbnails"
    },
    "GenerateThumbnails": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:FUNCTION",
      "Next": "Success"
    }
  }
}
```

### S3 + EventBridge (Advanced Routing)
**Advantages over S3 Event Notifications**:
- Content-based filtering (object size, metadata)
- Fan-out to 5+ targets
- Cross-account routing
- Integration with 20+ AWS services

**Example rule** (trigger on large CSV files):
```json
{
  "source": ["aws.s3"],
  "detail-type": ["Object Created"],
  "detail": {
    "bucket": {"name": ["my-data-bucket"]},
    "object": {
      "key": [{"suffix": ".csv"}],
      "size": [{"numeric": [">=", 100000000]}]
    }
  }
}
```

## 10.1) Hands‑on lab (pro‑level)
### Goal: Secure, optimized S3 bucket
1) Create bucket in `us‑east‑1`.
2) Enable **Block Public Access** + **Versioning**.
3) Set default **SSE‑KMS** encryption.
4) Create lifecycle: Standard → Standard‑IA at 30 days → Deep Archive at 365 days.
5) Create CRR to another region for DR.
6) Test access via IAM role (no static keys).

### Validate your setup (beginner checklist)
- Upload an object and confirm **versioning** creates versions.
- Confirm **default encryption** by checking object properties.
- Try a **public GET** and confirm it is blocked.
- Move the object forward in lifecycle (use a short test rule).

## 10.2) Create a fully configured S3 bucket (Terraform, AWS CLI, CloudFormation)
Below is a **production‑style baseline**: private bucket, versioning, SSE‑KMS, lifecycle, public access block, and access logging.

### Terraform (HCL)
```hcl
provider "aws" {
  region = "us-east-1"
}

resource "aws_kms_key" "s3_key" {
  description             = "S3 default encryption key"
  deletion_window_in_days = 30
  enable_key_rotation     = true
}

resource "aws_s3_bucket" "logs" {
  bucket = "my-company-s3-access-logs-123456"
}

resource "aws_s3_bucket" "main" {
  bucket = "my-company-data-123456"
}

resource "aws_s3_bucket_public_access_block" "main" {
  bucket                  = aws_s3_bucket.main.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_versioning" "main" {
  bucket = aws_s3_bucket.main.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "main" {
  bucket = aws_s3_bucket.main.id
  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "aws:kms"
      kms_master_key_id = aws_kms_key.s3_key.arn
    }
  }
}

resource "aws_s3_bucket_logging" "main" {
  bucket = aws_s3_bucket.main.id
  target_bucket = aws_s3_bucket.logs.id
  target_prefix = "s3-access/"
}

resource "aws_s3_bucket_lifecycle_configuration" "main" {
  bucket = aws_s3_bucket.main.id
  rule {
    id     = "lifecycle-standard-ia-glacier"
    status = "Enabled"
    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }
    transition {
      days          = 365
      storage_class = "DEEP_ARCHIVE"
    }
    noncurrent_version_expiration {
      noncurrent_days = 365
    }
  }
}
```

### AWS CLI
```bash
# 1) Create bucket
aws s3api create-bucket \
  --bucket my-company-data-123456 \
  --region us-east-1

# 2) Block public access
aws s3api put-public-access-block \
  --bucket my-company-data-123456 \
  --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

# 3) Enable versioning
aws s3api put-bucket-versioning \
  --bucket my-company-data-123456 \
  --versioning-configuration Status=Enabled

# 4) Create KMS key
KEY_ID=$(aws kms create-key --description "S3 default encryption key" --query KeyMetadata.KeyId --output text)
aws kms enable-key-rotation --key-id "$KEY_ID"

# 5) Default SSE-KMS encryption
aws s3api put-bucket-encryption \
  --bucket my-company-data-123456 \
  --server-side-encryption-configuration '{
    "Rules":[{
      "ApplyServerSideEncryptionByDefault":{
        "SSEAlgorithm":"aws:kms",
        "KMSMasterKeyID":"'"$KEY_ID"'"
      }
    }]
  }'

# 6) Access logs (requires log bucket)
aws s3api create-bucket --bucket my-company-s3-access-logs-123456 --region us-east-1
aws s3api put-bucket-logging \
  --bucket my-company-data-123456 \
  --bucket-logging-status '{
    "LoggingEnabled":{
      "TargetBucket":"my-company-s3-access-logs-123456",
      "TargetPrefix":"s3-access/"
    }
  }'

# 7) Lifecycle rules
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-company-data-123456 \
  --lifecycle-configuration '{
    "Rules":[{
      "ID":"lifecycle-standard-ia-glacier",
      "Status":"Enabled",
      "Transitions":[
        {"Days":30,"StorageClass":"STANDARD_IA"},
        {"Days":365,"StorageClass":"DEEP_ARCHIVE"}
      ],
      "NoncurrentVersionExpiration":{"NoncurrentDays":365}
    }]
  }'
```

### CloudFormation (YAML)
```yaml
AWSTemplateFormatVersion: "2010-09-09"
Description: S3 bucket with encryption, versioning, lifecycle, and public access block

Resources:
  S3KmsKey:
    Type: AWS::KMS::Key
    Properties:
      Description: S3 default encryption key
      EnableKeyRotation: true

  AccessLogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: my-company-s3-access-logs-123456

  MainBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: my-company-data-123456
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: aws:kms
              KMSMasterKeyID: !Ref S3KmsKey
      LifecycleConfiguration:
        Rules:
          - Id: lifecycle-standard-ia-glacier
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
              - TransitionInDays: 365
                StorageClass: DEEP_ARCHIVE
            NoncurrentVersionExpiration:
              NoncurrentDays: 365
      LoggingConfiguration:
        DestinationBucketName: !Ref AccessLogsBucket
        LogFilePrefix: s3-access/
```
## 11) FAANG‑level interview questions with beginner‑friendly explanations
> These include **tricky edge cases** often used in interviews. Each answer explains the “why” and trade‑offs in plain language.

### Q1) Design a multi‑tenant SaaS file store using S3 with tenant isolation (prevent privilege escalation)
**Beginner idea**: Tenants must not see each other’s files, even if they share a bucket.

**Two common designs (table)**  

| Model | How it works | Pros | Cons | Use when |
|---|---|---|---|---|
| **Bucket per tenant** | Each tenant has its own bucket | Strong isolation, simpler policy | Many buckets to manage | High‑compliance or large enterprise tenants |
| **Prefix per tenant** | Single bucket, `tenant-id/` prefix | Scales well, cheaper ops | Must enforce strict IAM | Most SaaS apps |

**Recommended (prefix model, safer)**  
1) One bucket with **Block Public Access** enabled.  
2) **IAM policy** restricts to `s3:prefix = tenant-id/*` and limits `s3:ListBucket`.  
3) Use **pre‑signed URLs** for upload/download.  
4) Encrypt with **SSE‑KMS** (optional: one CMK per tenant).  
5) Enable **CloudTrail** + **S3 access logs** for audit.  
6) Prevent confused‑deputy with `aws:SourceArn` or `aws:PrincipalArn` conditions.  

**Why this is FAANG‑level**: It balances scale, cost, and isolation without operational sprawl or escalation paths.

### Q2) How do you guarantee ransomware recovery when attackers get admin credentials?
**Beginner idea**: Prevent deletion and keep older copies.

**Solution**  
- **Enable versioning**.  
- Enable **Object Lock** in **Compliance** mode.  
- Add a **bucket policy deny** for deletes except for a break‑glass role.  
- Use **CRR** to a separate region and account.  

**Trick detail**: Compliance mode prevents deletion **even by admins** until retention expires.

### Q3) Compare SSE‑S3 vs SSE‑KMS vs client‑side encryption (with cost and audit trade‑offs)
**Beginner idea**: All encrypt at rest, but who controls keys differs.

| Method | Who owns keys | Auditability | Access control | Cost impact | When to use |
|---|---|---|---|---|---|
| **SSE‑S3** | AWS | Basic | Limited | Lowest | Simple workloads |
| **SSE‑KMS** | You | Strong (CloudTrail) | Fine‑grained | KMS request costs | Regulated workloads |
| **Client‑side** | You | Full control | Full control | Higher ops | Strict compliance |

### Q4) Design a 500 TB data lake where only 1% of data is queried monthly
**Beginner idea**: Keep recent data hot, move old data cold.

**Design**  
1) **Standard** for 0‑30 days.  
2) **Standard‑IA** for 31‑365 days.  
3) **Glacier Deep Archive** after 365 days.  
4) **Partition by date** for Athena.  
5) Use **Glue Catalog** + **Athena**.  

**Trick detail**: If queries often hit old data, use **Glacier IR** instead of Deep Archive.

### Q5) Explain the trade‑offs between Standard‑IA and Glacier Instant Retrieval with many small files
**Beginner idea**: Both allow instant access, but pricing differs.

| Factor | Standard‑IA | Glacier Instant Retrieval |
|---|---|---|
| Access time | ms | ms |
| Storage cost | Higher | Lower |
| Retrieval cost | Lower | Higher |
| Min storage duration | 30 days | 90 days |
| Min billable size | 128 KB | 128 KB |

**Rule of thumb**  
- **Standard‑IA** if you have **many small files** or occasional reads.  
- **Glacier IR** if reads are **very rare** and objects are larger.  

### Q6) Bucket has Block Public Access ON but data is still public. How?
**Beginner idea**: Block Public Access should prevent public access, but there are edge cases.

**Likely causes**  
- Public access is served **via CloudFront**, not directly from S3.  
- Block Public Access is **disabled at account level**.  
- Data is served from a **replica bucket** with different settings.  

**Trick detail**: CloudFront can make content public while the S3 bucket remains private.

### Q7) How do you enforce SSE‑KMS on every upload even if developers forget?
**Beginner idea**: Deny unencrypted PUTs.

**Solution**  
- Bucket policy denies `s3:PutObject` unless header `s3:x-amz-server-side-encryption = aws:kms`.  
- Optionally enforce a **specific KMS key ARN**.  

**Trick detail**: Multipart uploads must include encryption headers on each part.

### Q8) CRR works for some objects but fails for others. Why?
**Beginner idea**: Replication isn’t always automatic.

**Common causes**  
- Versioning is disabled on destination bucket.  
- KMS key policy doesn’t allow the replication role.  
- Replication rule doesn’t match object prefix/tags.  

**Trick detail**: KMS permissions are the #1 hidden cause of CRR failures.

### Q9) User can list objects but cannot download them. What’s wrong?
**Beginner idea**: `ListBucket` is not the same as `GetObject`.

**Answer**  
- `s3:ListBucket` is allowed, but `s3:GetObject` is missing.  
- Prefix conditions allow list but object ARN permissions are absent.  

**Trick detail**: Bucket policy needs **both** bucket‑level and object‑level permissions.

### Q10) How do you meet strict data‑residency rules while using CloudFront?
**Beginner idea**: Keep data in the required region and control caching.

**Solution**  
- Keep the S3 origin in the required region.  
- Use **CloudFront with minimal caching** or restrict cache for sensitive content.  
- Use **Origin Shield** in the same region.  

**Trick detail**: CloudFront caches data at edge locations; strict residency may require disabling caching or using regional endpoints only.

### Q11) Uploads fail with “AccessDenied: KMS key not accessible” even though IAM allows s3:PutObject. Why?
**Beginner idea**: S3 permissions are not enough when using SSE‑KMS.

**Answer**  
- The **KMS key policy** must allow the caller (or role) to use `kms:Encrypt`.  
- The **bucket policy** might require a specific KMS key and the request uses a different key.  

**Trick detail**: KMS access is controlled by **both** IAM policy and the **KMS key policy**.

### Q12) An app uses pre‑signed URLs to upload, but uploads sometimes fail after 5–10 minutes. Why?
**Beginner idea**: Pre‑signed URLs expire and are time‑sensitive.

**Answer**  
- URL **expiration time** might be too short.  
- **Clock skew** between client and AWS can invalidate signatures.  
- Multipart uploads require **signed headers** for each part.  

**Trick detail**: Large uploads should use multipart pre‑signed URLs with longer expiry.

### Q13) After enabling Object Lock, the security team still deleted files. How?
**Beginner idea**: Object Lock only works with versioning and correct mode.

**Answer**  
- Bucket **was not created with Object Lock enabled** at creation time.  
- Object Lock in **Governance mode** allows privileged users to bypass retention.  

**Trick detail**: Only **Compliance mode** prevents deletion by admins.

### Q14) A bucket policy denies unencrypted uploads, but multipart uploads still succeed without SSE‑KMS. Why?
**Beginner idea**: Multipart uploads need special handling.

**Answer**  
- The policy likely checks headers on **PutObject** but not on **CreateMultipartUpload** or **UploadPart**.  

**Fix**  
- Enforce encryption on **CreateMultipartUpload** requests and require `x-amz-server-side-encryption` headers.  

**Trick detail**: Multipart uploads are separate API calls with their own headers.

### Q15) Why does `s3:ListBucket` allow a user to see keys from other tenants even with prefix conditions?
**Beginner idea**: Prefix conditions must be exact.

**Answer**  
- Prefix condition was too broad (e.g., `tenant-1*` also matches `tenant-12`).  
- Missing `s3:delimiter` caused full listing.  

**Trick detail**: Use exact prefix matching and strict delimiter conditions to avoid leakage.

## 12) Certification‑style questions (20) with explanations
1) Which S3 feature protects against accidental deletes?  
A) CORS  B) Versioning  C) ACLs  D) Bucket policy  
Answer: B  
Why: Versioning keeps previous object versions and delete markers.  
Why not:  
- A: CORS controls browser access, not deletion protection.  
- C: ACLs only control access.  
- D: Bucket policy controls access, not version retention.  

2) Which storage class is lowest cost for long‑term archives?  
A) Standard‑IA  B) Glacier IR  C) Glacier Deep Archive  D) One Zone‑IA  
Answer: C  
Why: Deep Archive is designed for lowest‑cost, long‑term retention.  
Why not:  
- A/B/D: Higher storage cost than Deep Archive.  

3) Which feature blocks public access across all buckets?  
A) IAM role  B) Block Public Access  C) ACL  D) Route 53  
Answer: B  
Why: Block Public Access is an account‑level and bucket‑level guardrail.  
Why not:  
- A: IAM role affects principals, not bucket public exposure globally.  
- C: ACLs are per‑object/bucket and can still allow public access.  
- D: Route 53 is DNS, unrelated.  

4) A company is planning to migrate to AWS and wants to become more responsive to customer inquiries and feedback. Which AWS CAF tasks should it perform? (Select TWO.)  
A) Create new value propositions with new products and services  
B) Use agile methods to rapidly iterate and evolve  
C) Migrate and modernize legacy infrastructure  
D) Use a new data and analytics platform to create actionable insights  
E) Organize teams around products and value streams  
Answer: B, E  
Why: Agile iteration and product/value‑stream teams are core to organizational transformation and responsiveness.  
Why not:  
- A/D: Value propositions and analytics are business/data tasks, but not the primary CAF org transformation actions for responsiveness.  
- C: Migration/modernization is technical, not organizational.  

5) A company needs shared file storage mounted by many Linux EC2 instances. Which AWS service fits best?  
A) S3  B) EBS  C) EFS  D) Glacier  
Answer: C  
Why: EFS is managed NFS, shared across instances.  
Why not:  
- A: S3 is object storage, not a POSIX file system.  
- B: EBS is block storage attached to one instance.  
- D: Glacier is archival storage.  

6) Which statement about S3 consistency is correct?  
A) S3 is eventually consistent for new objects  
B) S3 is strongly consistent for read‑after‑write and list  
C) S3 is only consistent within a single AZ  
D) S3 consistency depends on storage class  
Answer: B  
Why: S3 provides strong read‑after‑write consistency for all operations.  
Why not:  
- A/C/D: Outdated or incorrect statements.  

7) Which requirement best fits S3 Standard‑IA?  
A) Frequent access with very low latency  
B) Infrequent access with instant retrieval  
C) Rare access with hours of retrieval time  
D) Data that must stay in a single AZ only  
Answer: B  
Why: Standard‑IA is for infrequent access but still millisecond retrieval.  
Why not:  
- A: Use S3 Standard.  
- C: Use Glacier Flexible/Deep Archive.  
- D: One Zone‑IA is single‑AZ, not Standard‑IA.  

8) Which S3 feature lets you privately access S3 without an internet gateway or NAT?  
A) VPC peering  B) VPC endpoint (Gateway)  C) Direct Connect  D) CloudFront  
Answer: B  
Why: Gateway VPC endpoints provide private S3 access within a VPC.  
Why not:  
- A/C: Network connectivity but not direct S3 private access.  
- D: CloudFront is a CDN.  

9) Which is the best way to grant temporary upload access to external users?  
A) Make the bucket public  
B) Use pre‑signed URLs  
C) Use ACLs for everyone  
D) Create IAM users for each external user  
Answer: B  
Why: Pre‑signed URLs give time‑limited, scoped access.  
Why not:  
- A/C: Too broad and insecure.  
- D: High ops overhead and not best practice.  

10) Which S3 storage class is best for data accessed once or twice per year but must be instantly available?  
A) Standard  B) Standard‑IA  C) Glacier Instant Retrieval  D) Glacier Deep Archive  
Answer: C  
Why: Glacier IR is optimized for very infrequent but instant access.  
Why not:  
- A/B: Higher storage costs.  
- D: Retrieval is hours, not instant.  

11) Which must be enabled to use S3 Cross‑Region Replication (CRR)?  
A) MFA delete  B) Versioning  C) Lifecycle policies  D) Transfer Acceleration  
Answer: B  
Why: Versioning is required on source and destination buckets for CRR.  
Why not:  
- A/C/D: Not required.  

12) A user wants to host a static website with HTTPS using S3. What is the best approach?  
A) Enable S3 website hosting only  
B) Use CloudFront in front of S3  
C) Use EFS with EC2 web server  
D) Use Glacier Flexible Retrieval  
Answer: B  
Why: CloudFront provides HTTPS and better performance.  
Why not:  
- A: S3 website hosting alone is HTTP only.  
- C: Unnecessary complexity for static content.  
- D: Glacier is archival.  

13) Which feature ensures immutable storage for compliance (WORM)?  
A) Versioning  B) Object Lock  C) Lifecycle rules  D) Replication  
Answer: B  
Why: Object Lock enforces retention/legal hold.  
Why not:  
- A: Versioning keeps history but doesn’t enforce immutability.  
- C/D: Not immutability mechanisms.  

14) Which action reduces S3 cost for large archives automatically?  
A) Enable multipart upload  
B) Add lifecycle rules to move to Glacier  
C) Use larger objects only  
D) Turn off encryption  
Answer: B  
Why: Lifecycle transitions move data to cheaper classes automatically.  
Why not:  
- A/C: Not primary cost optimizers.  
- D: Security risk, not cost best practice.  

15) Which S3 encryption option gives the most auditability and access control?  
A) SSE‑S3  B) SSE‑KMS  C) SSE‑C  D) Client‑side without KMS  
Answer: B  
Why: KMS provides key policies and CloudTrail audit logs.  
Why not:  
- A: Less control.  
- C: You must manage keys; less centralized audit.  
- D: No AWS‑managed audit controls.  

16) A company has many small (<128 KB) files in Standard‑IA. What issue might they see?  
A) Higher compute cost  
B) Objects charged at minimum 128 KB per object  
C) No lifecycle support  
D) Inability to use encryption  
Answer: B  
Why: IA classes have a minimum billable size of 128 KB.  
Why not:  
- A/C/D: Not true.  

17) Which method minimizes upload time for a 10‑GB file?  
A) Single PUT  
B) Multipart upload  
C) Use Glacier upload  
D) Use S3 Select  
Answer: B  
Why: Multipart upload improves throughput and reliability.  
Why not:  
- A: Slower and less resilient.  
- C: Glacier is a storage class, not an upload method.  
- D: S3 Select is for querying object data.  

18) Which service is best for on‑premises file shares backed by S3?  
A) Storage Gateway (File Gateway)  
B) EFS  
C) EBS  
D) FSx for Lustre  
Answer: A  
Why: File Gateway exposes S3 as NFS/SMB to on‑premises.  
Why not:  
- B/C/D: Not designed for on‑prem gateway access to S3.  

19) Which statement about S3 Object Lock is correct?  
A) It works without versioning  
B) It can be bypassed by any IAM admin  
C) It provides WORM retention with compliance/governance modes  
D) It is only for Glacier classes  
Answer: C  
Why: Object Lock enforces WORM with compliance/governance retention.  
Why not:  
- A: Versioning is required.  
- B: Compliance mode prevents deletion even by admins until retention ends.  
- D: Works with S3 objects, not just Glacier.  

20) Which S3 feature lets you query a subset of an object (e.g., CSV rows) without downloading the entire object?  
A) S3 Transfer Acceleration  B) S3 Select  C) S3 Inventory  D) S3 Batch Operations  
Answer: B  
Why: S3 Select retrieves subset of data using SQL expressions.  
Why not:  
- A: Accelerates uploads/downloads.  
- C: Reports inventory, not query.  
- D: Performs large‑scale actions, not selective query.  

21) What happens to objects in Standard-IA if deleted before 30 days?
A) No charges  B) Early deletion fee applies  C) Object cannot be deleted  D) Automatic refund
Answer: B
Why: Deleting before minimum duration incurs early deletion charges for remaining days.
Why not:
- A/C/D: Incorrect; you pay for the full minimum duration.

22) Which encryption method provides independent audit logs for each key access?
A) SSE-S3  B) SSE-KMS  C) SSE-C  D) Client-side with your own keys
Answer: B
Why: SSE-KMS logs every encryption/decryption request in CloudTrail.
Why not:
- A: No per-request key audit trails.
- C/D: You manage keys; less AWS-integrated audit.

23) A bucket has 10 million objects averaging 50 KB each moved to Glacier Instant Retrieval. What cost issue might arise?
A) Retrieval failures  B) Minimum 128 KB billing per object  C) Latency increase  D) No SSE support
Answer: B
Why: Each object billed as 128 KB minimum, inflating cost (50 KB charged as 128 KB).
Why not:
- A/C/D: Not the primary cost concern for small objects.

24) Which S3 feature helps identify buckets without encryption or versioning across an entire AWS Organization?
A) CloudTrail  B) S3 Storage Lens  C) S3 Inventory  D) Config Rules
Answer: B
Why: S3 Storage Lens aggregates metrics across accounts and shows security posture.
Why not:
- A: Logs events, not aggregated compliance dashboards.
- C: Per-bucket inventory, not org-wide analysis.
- D: Config rules can check, but Storage Lens provides broader analytics.

25) When does S3 Transfer Acceleration provide the most benefit?
A) Uploads from same region  B) Uploads from geographically distant locations  C) Small file uploads  D) Glacier retrievals
Answer: B
Why: Transfer Acceleration uses CloudFront edge locations to speed uploads over long distances.
Why not:
- A: Minimal benefit in same region.
- C/D: Overkill for small files; doesn't apply to Glacier.

26) What is the maximum size of a single S3 object?
A) 5 GB  B) 5 TB  C) 100 GB  D) Unlimited
Answer: B
Why: Maximum object size is 5 TB.
Why not:
- A: 5 GB is the max for single PUT; use multipart for larger.
- C/D: Incorrect limits.

27) Which IAM permission is required to list objects in a bucket?
A) s3:GetObject  B) s3:ListBucket  C) s3:ListAllMyBuckets  D) s3:PutObject
Answer: B
Why: `s3:ListBucket` permission is needed on the bucket resource.
Why not:
- A: Gets object content, not listing.
- C: Lists all buckets in account, not objects in a bucket.
- D: Upload permission.

28) A company needs objects deleted automatically after 90 days. What should they use?
A) Object Lock  B) Lifecycle expiration rule  C) Lambda function  D) Versioning
Answer: B
Why: Lifecycle policy with expiration action automatically deletes objects.
Why not:
- A: Object Lock prevents deletion, opposite goal.
- C: Unnecessary complexity; lifecycle is built-in.
- D: Versioning keeps deleted objects as delete markers.

29) Which S3 replication feature guarantees 99.99% of objects replicate within 15 minutes?
A) Standard CRR  B) Replication Time Control (RTC)  C) SRR  D) Batch replication
Answer: B
Why: RTC provides SLA for replication time.
Why not:
- A/C: No time SLA without RTC.
- D: Not a feature; S3 Batch Operations can backfill.

30) What is the purpose of S3 Inventory?
A) Track user activity  B) Generate object metadata reports  C) Monitor bucket size  D) Encrypt objects
Answer: B
Why: S3 Inventory creates scheduled reports of object metadata (size, storage class, encryption status).
Why not:
- A: CloudTrail tracks activity.
- C: CloudWatch tracks size.
- D: Encryption is a separate feature.

31) Which VPC feature allows private S3 access without internet gateway?
A) NAT Gateway  B) VPC Endpoint (Gateway)  C) VPN  D) Direct Connect
Answer: B
Why: S3 Gateway VPC Endpoint provides private access to S3 within VPC.
Why not:
- A/C/D: Network connectivity options, not direct S3 endpoint.

32) A user can PutObject but the object does not appear in ListBucket results. Why?
A) Eventual consistency  B) IAM policy lacks ListBucket  C) Object is encrypted  D) Wrong storage class
Answer: B
Why: User has PutObject permission but not ListBucket on the bucket.
Why not:
- A: S3 is now strongly consistent.
- C/D: Irrelevant to listing.

33) What happens to S3 data during an AZ failure?
A) Data loss  B) Automatic failover to another AZ  C) Manual restore required  D) Degraded performance only
Answer: B
Why: S3 Standard replicates across multiple AZs and automatically handles AZ failures.
Why not:
- A: 11 nines durability prevents data loss.
- C/D: Automatic recovery, no manual action or degradation for Standard.

34) Which S3 feature prevents accidental deletion of critical objects even by root user?
A) Versioning  B) MFA Delete + Versioning  C) Object Lock Compliance mode  D) Bucket policy deny
Answer: C
Why: Compliance mode prevents deletion until retention period expires, even by root.
Why not:
- A: Protects via versioning but doesn't prevent deletion.
- B: Requires MFA but can still be deleted.
- D: Policies can be changed by admins.

35) How can you reduce S3 costs for a data lake with 80% of data never accessed after 60 days?
A) Delete old data  B) Lifecycle transition to IA/Glacier  C) Use smaller objects  D) Disable versioning
Answer: B
Why: Lifecycle rules automatically move infrequently accessed data to cheaper storage classes.
Why not:
- A: May need data for compliance.
- C/D: Don't address access patterns.

36) What is the difference between Object Lock Governance vs Compliance mode?
A) Governance is cheaper  B) Governance allows privileged users to override retention  C) Compliance allows early deletion  D) No difference
Answer: B
Why: Governance mode allows users with special permissions to bypass retention; Compliance mode does not.
Why not:
- A/C/D: Incorrect characterizations.

37) A company needs to enforce encryption at rest for all new objects. What's the best approach?
A) IAM policy  B) Bucket default encryption + bucket policy deny unencrypted uploads  C) Lambda function  D) Use ACLs
Answer: B
Why: Default encryption ensures all objects are encrypted; bucket policy enforces the requirement.
Why not:
- A: IAM controls principals, not bucket-level enforcement.
- C: Unnecessary complexity.
- D: ACLs don't enforce encryption.

38) Which S3 API call is most expensive in terms of request costs?
A) GET  B) PUT  C) LIST  D) HEAD
Answer: C
Why: LIST operations are typically more expensive than GET. PUT/COPY/POST are also higher cost.
Why not:
- A/D: GET and HEAD are cheaper tier requests.

39) What happens when you enable MFA Delete on a bucket?
A) All users need MFA for uploads  B) MFA required to delete object versions or disable versioning  C) MFA required for all API calls  D) Bucket becomes read-only
Answer: B
Why: MFA Delete requires MFA token to delete versions or change versioning state.
Why not:
- A/C/D: MFA Delete only applies to destructive versioning operations.

40) A company stores medical records in S3 and must retain them unmodified for 10 years. Which features should they use?
A) Versioning only  B) Lifecycle rules only  C) Object Lock Compliance + Versioning  D) Glacier only
Answer: C
Why: Object Lock Compliance mode enforces WORM (write-once-read-many) for regulatory compliance.
Why not:
- A/B: Don't enforce immutability.
- D: Glacier is storage class, not retention enforcement.

41) How does S3 achieve 99.999999999% (11 nines) durability?
A) Single AZ replication  B) Multi-AZ replication with erasure coding  C) Manual backups  D) RAID arrays
Answer: B
Why: S3 replicates data across multiple devices in multiple AZs using erasure coding.
Why not:
- A/C/D: Don't provide 11 nines durability.

42) Which scenario is best for One Zone-IA instead of Standard-IA?
A) Critical data  B) Reproducible data tolerable to AZ loss  C) Frequently accessed data  D) Multi-region replication source
Answer: B
Why: One Zone-IA is cheaper but stores in single AZ; use for non-critical, reproducible data.
Why not:
- A/C/D: Use Standard or Standard-IA.

43) What is the purpose of S3 Access Points?
A) Speed up uploads  B) Simplify permissions with application-specific endpoints  C) Encrypt objects  D) Monitor access
Answer: B
Why: Access Points provide named network endpoints with dedicated access policies per application.
Why not:
- A/C/D: Not the primary purpose.

44) A bucket policy allows public read, but objects are still denied. What could be the reason?
A) Bucket versioning  B) Block Public Access is enabled  C) Encryption  D) Lifecycle rules
Answer: B
Why: Block Public Access overrides bucket policies that allow public access.
Why not:
- A/C/D: Don't block public access.

45) Which service provides on-premises file share access backed by S3?
A) EFS  B) Storage Gateway File Gateway  C) Direct Connect  D) DataSync
Answer: B
Why: Storage Gateway File Gateway exposes S3 as NFS/SMB to on-premises systems.
Why not:
- A: Cloud-native file system.
- C: Network connectivity, not storage gateway.
- D: Data transfer service, not a file share.

46) How can you retrieve a specific version of an object in a versioned bucket?
A) Use object key only  B) Specify version ID in GET request  C) Cannot retrieve old versions  D) Use lifecycle policy
Answer: B
Why: Specify `versionId` parameter in GET request to retrieve specific version.
Why not:
- A: Returns latest version only.
- C/D: Incorrect.

47) What is S3 Intelligent-Tiering best used for?
A) Known access patterns  B) Unknown or changing access patterns  C) Archive-only data  D) Publicly accessible content
Answer: B
Why: Intelligent-Tiering automatically moves objects between access tiers based on usage.
Why not:
- A: Use explicit storage class if pattern is known.
- C: Use Glacier classes.
- D: Unrelated to access control.

48) Which S3 feature allows SQL queries on object data without downloading?
A) S3 Select  B) Athena  C) Redshift Spectrum  D) All of the above
Answer: D
Why: All three services enable SQL queries on S3 data; S3 Select works on individual objects.
Why not:
- Individual options all valid, but "all of the above" is most complete.

49) What is the recommended upload method for objects larger than 100 MB?
A) Single PUT  B) Multipart upload  C) POST request  D) FTP
Answer: B
Why: Multipart upload improves throughput, reliability, and enables parallel uploads for large files.
Why not:
- A: Works but less reliable/slower for large files.
- C/D: Not S3 best practices.

50) What is the effect of enabling S3 Versioning on costs?
A) No cost change  B) Cost increases based on number of versions stored  C) Cost decreases  D) Free feature
Answer: B
Why: Each version is billed as a separate object; storage costs increase with version count.
Why not:
- A/C/D: additional storage = additional cost.

51) A bucket has CRR enabled, but new objects are not replicating. Which is the LEAST likely cause?
A) Destination bucket versioning disabled  B) IAM replication role lacks permissions  C) Object is too large  D) KMS key policy issue
Answer: C
Why: S3 supports large object replication; size is not typically the blocker.
Why not:
- A/B/D: Common causes of replication failure.

52) Which S3 storage class has the lowest storage cost per GB?
A) Standard  B) Glacier Instant Retrieval  C) Glacier Deep Archive  D) One Zone-IA
Answer: C
Why: Glacier Deep Archive has the lowest per-GB storage cost.
Why not:
- A/B/D: Higher storage costs than Deep Archive.

53) What does "strong read-after-write consistency" mean in S3?
A) Eventual consistency for all operations  B) Immediate visibility of new/updated objects in all read requests  C) Consistency only within same AZ  D) Requires versioning
Answer: B
Why: S3 provides immediate read consistency after writes for all operations.
Why not:
- A/C/D: Outdated or incorrect.

54) Which resource policy controls access to S3 objects?
A) IAM policy  B) Bucket policy  C) Security group  D) Network ACL
Answer: B
Why: Bucket policies are resource-based policies that control access to S3 buckets and objects.
Why not:
- A: IAM policies are identity-based.
- C/D: Network controls, not S3-specific.

55) How can you automatically delete incomplete multipart uploads after 7 days?
A) Lambda function  B) Lifecycle rule with abort incomplete multipart upload  C) S3 Batch Operations  D) Manual deletion
Answer: B
Why: Lifecycle policies can abort incomplete multipart uploads after a specified period.
Why not:
- A/C/D: Unnecessary or manual effort.

56) Which scenario requires S3 Request Metrics (not just daily storage metrics)?
A) Track bucket size growth  B) Near real-time monitoring of request rates and latency  C) Monthly billing reports  D) Lifecycle transitions
Answer: B
Why: Request Metrics provide 1-minute CloudWatch data for requests, errors, and latency.
Why not:
- A/C/D: Covered by standard metrics or billing.

57) What is the maximum number of S3 buckets per AWS account by default?
A) 10  B) 100  C) 1000  D) Unlimited
Answer: B
Why: Default limit is 100 buckets per account (can request increase).
Why not:
- A/C/D: Incorrect default limits.

58) Which access control method is AWS's recommended approach for S3?
A) ACLs  B) Bucket policies and IAM policies  C) Public access  D) Pre-signed URLs only
Answer: B
Why: AWS recommends using bucket policies and IAM policies over legacy ACLs.
Why not:
- A: ACLs are legacy; limited functionality.
- C/D: Too broad or limited use cases.

59) How does S3 Object Lambda modify data?
A) Pre-processing before write  B) During GET requests via Lambda function  C) Scheduled batch processing  D) Automatic compression
Answer: B
Why: S3 Object Lambda transforms objects on-the-fly during GET requests using Lambda.
Why not:
- A/C/D: Different processes, not Object Lambda mechanism.

60) A company needs to delete 10 billion S3 objects. What is the most efficient method?
A) AWS CLI delete loop  B) S3 Batch Operations  C) Lambda function  D) Lifecycle expiration
Answer: B (or D if time-based)
Why: S3 Batch Operations handles large-scale object operations efficiently. Lifecycle expiration is best if deletion is time-based.
Why not:
- A: Too slow and error-prone at scale.
- C: Less scalable than Batch Operations.

## 13) Quick recap (beginner → pro)
- Use S3 for **object storage**, not file systems or block storage.
- Choose storage class based on **access pattern and retrieval time**.
- Enforce security with **Block Public Access + IAM + bucket policy + KMS**.
- Use **lifecycle rules** to cut cost automatically.
- Expect interviewers to test **edge cases** (KMS policy, Object Lock, prefix leakage).

## 14) Performance optimization deep dive

### Upload optimization strategies

| File Size | Recommended Method | Parallelization | Use Case |
|---|---|---|---|
| < 100 MB | Single PUT | No | Small files, simple uploads |
| 100 MB - 5 GB | Multipart upload | Yes (optional) | Improved reliability |
| > 5 GB | Multipart upload | Yes (required) | Large files, required by S3 |

**Multipart upload benefits**:
- Parallel part uploads (up to 10,000 parts)
- Resume failed uploads without restarting
- Upload while file is being created
- Each part: 5 MB to 5 GB (except last part)

**Example** (Python SDK):
```python
import boto3
from boto3.s3.transfer import TransferConfig

s3 = boto3.client('s3')
transfer_config = TransferConfig(
    multipart_threshold=100*1024*1024,  # 100 MB
    max_concurrency=10,
    multipart_chunksize=10*1024*1024,   # 10 MB per part
    use_threads=True
)

s3.upload_file(
    'large_file.dat', 
    'my-bucket', 
    'key',
    Config=transfer_config
)
```

### Download optimization

**Use byte-range fetches for parallel downloads**:
```bash
# Download parts in parallel
aws s3api get-object --bucket my-bucket --key file.dat --range bytes=0-10485759 part1 &
aws s3api get-object --bucket my-bucket --key file.dat --range bytes=10485760-20971519 part2 &
# Combine parts
cat part1 part2 > file.dat
```

**CloudFront for frequent access**:
- Cache at edge locations globally
- Reduce S3 GET request costs
- Lower latency for end users

### Request rate performance

**Historical limit** (before 2018): 100 PUT/LIST/DELETE per second per prefix

**Current** (2020+): **3,500 PUT/COPY/POST/DELETE** and **5,500 GET/HEAD** per second per prefix

**Best practices**:
- No special prefixing needed for most workloads
- If you need >5,500 requests/second, use multiple prefixes
- Avoid sequential key names if workload is extreme (>10,000 req/sec)

**Example prefix strategy for high throughput**:
```
# Instead of sequential:
logs/2026-01-30-00001.log
logs/2026-01-30-00002.log

# Use hash prefix for extreme scale:
a1b2/logs/2026-01-30-00001.log
c3d4/logs/2026-01-30-00002.log
```

### S3 Transfer Acceleration

**How it works**: Upload to nearest CloudFront edge location, which routes over AWS backbone to S3 bucket.

**When to use**:
- Uploads from geographically distant locations
- Users in multiple countries uploading to single region
- Variability in network conditions

**Cost**: +$0.04/GB for US/EU/Japan edge locations

**Speed comparison tool**: AWS provides speed comparison before enabling

**Enable via CLI**:
```bash
aws s3api put-bucket-accelerate-configuration \
  --bucket my-bucket \
  --accelerate-configuration Status=Enabled

# Upload using acceleration endpoint
aws s3 cp file.dat s3://my-bucket/ --endpoint-url https://my-bucket.s3-accelerate.amazonaws.com
```

## 15) Cost optimization strategies (FinOps best practices)

### Priority 1: Storage class optimization

**Checklist**:
1. Run **S3 Storage Class Analysis** for 30-90 days
2. Create **lifecycle rules** based on access patterns
3. Use **Intelligent-Tiering** for unknown patterns (monitoring fee: $0.0025 per 1,000 objects/month)
4. Avoid storing small objects in IA/Glacier classes (128 KB minimum billing)

**Example savings**:
```
100 TB data rarely accessed:
- Standard: $2,300/month
- Standard-IA: $1,250/month (46% savings)
- Glacier Deep Archive: $99/month (96% savings)
```

### Priority 2: Request optimization

**Common waste patterns**:
- Polling S3 for new objects (use S3 Event Notifications instead)
- Listing entire buckets repeatedly (use CloudWatch Events or prefix caching)
- Small object proliferation (consolidate into larger objects where possible)

**Cost comparison** (1 million requests):
- GET: $0.40 per million (Standard)
- PUT: $5.00 per million (Standard)
- LIST: $5.00 per million

### Priority 3: Data transfer optimization

**Strategies**:
- Use CloudFront to reduce S3 egress (first 100 GB/month free) - Use S3 Transfer Acceleration only when measurable benefit
- Keep processing in same region (S3 to EC2/Lambda in same region = free)
- Use VPC endpoints to avoid NAT Gateway costs

**Data transfer costs** (us-east-1 to internet):
- First 100 GB/month: $0.00
- Next 10 TB/month: $0.09/GB
- Over 150 TB/month: $0.05/GB

### Priority 4: Lifecycle and cleanup

**Auto-cleanup policies**:
```bash
# Delete incomplete multipart uploads after 7 days
{
  "Rules": [{
    "ID": "cleanup-multipart",
    "Status": "Enabled",
    "AbortIncompleteMultipartUpload": {
      "DaysAfterInitiation": 7
    }
  }]
}

# Delete non-current versions after 30 days
{
  "Rules": [{
    "ID": "cleanup-old-versions",
    "Status": "Enabled",
    "NoncurrentVersionExpiration": {
      "NoncurrentDays": 30
    }
  }]
}
```

### Priority 5: Monitoring and alerting

**Set up CloudWatch alarms**:
- Bucket size growth anomalies
- Request rate spikes
- 4xx/5xx error rate increases
- Data transfer out exceeding budget

**Use Cost Allocation Tags**:
```bash
aws s3api put-bucket-tagging --bucket my-bucket --tagging '{
  "TagSet": [
    {"Key": "Team", "Value": "DataScience"},
    {"Key": "CostCenter", "Value": "Engineering"},
    {"Key": "Environment", "Value": "Production"}
  ]
}'
```

## 16) S3 limits and quotas (certification reference)

### Hard limits (cannot be changed)

| Limit | Value |
|---|---|
| Max object size | 5 TB |
| Max single PUT size | 5 GB |
| Max multipart parts | 10,000 |
| Min multipart part size | 5 MB (except last part) |
| Max multipart part size | 5 GB |
| Max metadata size | 2 KB (user metadata), 8 KB (total) |
| Max key length | 1,024 bytes (UTF-8) |
| Max tags per object | 10 |

### Soft limits (can request increase)

| Limit | Default | Notes |
|---|---|---|
| Buckets per account | 100 | Increase to 1,000 possible |
| Event notifications per bucket | 100 | Combined S3/SNS/SQS/Lambda |
| Lifecycle rules per bucket | 1,000 | |
| Replication rules per bucket | 1,000 | |

### Performance limits (per prefix)

| Operation | Requests/second |
|---|---|
| GET/HEAD | 5,500 |
| PUT/COPY/POST/DELETE | 3,500 |
| LIST | No specific limit, latency increases with bucket size |

> [!NOTE]
> These limits are per prefix. Use multiple prefixes to scale beyond these limits.

## 17) Common anti-patterns and how to avoid them

### Anti-pattern 1: Using S3 as a database
**Problem**: Frequent small updates, high request costs, no ACID transactions

**Instead use**:
- DynamoDB for key-value with frequent updates
- RDS for relational data
- ElastiCache for caching layer

### Anti-pattern 2: Storing millions of tiny objects in IA/Glacier
**Problem**: Minimum 128 KB billing + per-object overhead

**Fix**:
- Combine small objects into larger archives (TAR/ZIP)
- Use Standard class for small, frequently accessed objects
- Example: 1 million 10 KB files in Standard-IA costs same as 128 KB each = 128 GB billing (vs 10 GB actual)

### Anti-pattern 3: Public buckets for private data
**Problem**: Security vulnerability, compliance violation

**Fix**:
- Always enable **Block Public Access**
- Use pre-signed URLs for temporary access
- Use CloudFront with OAC for controlled distribution

### Anti-pattern 4: Not using lifecycle policies
**Problem**: Paying Standard prices for cold data

**Fix**:
- Set up lifecycle transitions based on access patterns
- Use S3 Storage Class Analysis to determine optimal transitions

### Anti-pattern 5: Ignoring versioning costs
**Problem**: Unbounded cost growth from accumulating versions

**Fix**:
- Set lifecycle rules to expire non-current versions after N days
- Use MFA Delete for critical buckets
- Monitor version count with S3 Storage Lens

### Anti-pattern 6: Using ACLs instead of policies
**Problem**: ACLs are legacy, limited, harder to manage at scale

**Fix**:
- Use IAM and bucket policies
- Disable ACLs entirely (S3 Object Ownership = Bucket Owner Enforced)

### Anti-pattern 7: Same-region replication without purpose
**Problem**: Doubling storage costs without clear benefit

**Fix**:
- Use SRR only when justified (compliance, log aggregation, different encryption)
- Consider if lifecycle/versioning achieves same goal more cheaply

### Anti-pattern 8: No monitoring or Cost Allocation Tags
**Problem**: Cannot track or optimize costs by team/project

**Fix**:
- Tag all buckets with Team, Environment, CostCenter
- Enable Cost Allocation Tags in billing console
- Use Cost Explorer to analyze S3 costs by tag

## 13) Quick recap (beginner → pro)
